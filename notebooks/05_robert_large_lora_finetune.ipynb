{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f272a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s38976581_gmail_com/micromamba/envs/fin_sentiment/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "import ml_collections\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "from utils import clean_text, preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67982814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the model hyperparameters\n",
    "datetime_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "def model_config():\n",
    "    cfg_dictionary = {\n",
    "        \"data_path\": \"../data/data.csv\",\n",
    "        \"test_split_size\": 0.2,\n",
    "        \"validation_split_size\":0.2,\n",
    "                \n",
    "        \"train_batch_size\": 32,\n",
    "        \"eval_batch_size\": 32,\n",
    "\n",
    "        \"epochs\": 5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"lr\": 1e-4,  # Higher learning rate for LoRA (typically 1e-4 to 3e-4)\n",
    "        \"num_warmup_steps\": 10,\n",
    "\n",
    "        \"max_length\": 128,\n",
    "        \"random_seed\": 42,\n",
    "        \"num_labels\": 3,\n",
    "        \"model_checkpoint\":\"FacebookAI/roberta-large\",\n",
    "\n",
    "        # LoRA hyperparameters\n",
    "        \"lora_r\": 16,  # Rank of the low-rank matrices\n",
    "        \"lora_alpha\": 32,  # Scaling factor (typically 2x lora_r)\n",
    "        \"lora_dropout\": 0.1,  # Dropout for LoRA layers\n",
    "    }\n",
    "    cfg = ml_collections.FrozenConfigDict(cfg_dictionary)\n",
    "\n",
    "    return cfg\n",
    "cfg = model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea963a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataframe):\n",
    "    train_df, test_df = train_test_split(\n",
    "        dataframe,\n",
    "        test_size=cfg.test_split_size,\n",
    "        random_state=cfg.random_seed,\n",
    "        stratify=dataframe.labels.values,\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size=cfg.validation_split_size,\n",
    "        random_state=cfg.random_seed,\n",
    "        stratify=train_df.labels.values,\n",
    "    )\n",
    "\n",
    "    dataset = {\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"validation\": Dataset.from_pandas(val_df),\n",
    "        \"test\": Dataset.from_pandas(test_df),\n",
    "    }\n",
    "\n",
    "    dataset = DatasetDict(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b75963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(csv_file: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "    df[\"labels\"] = labelencoder.fit_transform(df[\"Sentiment\"])\n",
    "    df.drop_duplicates(subset=['Sentence'],keep='first',inplace=True)\n",
    "\n",
    "    cleaned_df = clean_text(df, \"Sentence\")\n",
    "    df.rename(columns={\"Sentiment\": \"sentiment\"}, inplace=True)\n",
    "    df.rename(columns={\"Sentence\": \"sentence\"}, inplace=True)\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "def tokenize_dataset():\n",
    "    dataset = create_dataset(preprocess_csv(cfg.data_path))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_checkpoint,use_fast=True)\n",
    "\n",
    "    def tokenize_function(sample):\n",
    "        outputs = tokenizer(\n",
    "            sample[\"sentence\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=cfg.max_length,\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=[\"sentence\",\"sentiment\",\"__index_level_0__\"]\n",
    "    )\n",
    "    # Rename 'label' to 'labels' as expected by HuggingFace models\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c22ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(tokenized_datasets):\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"], shuffle=True, batch_size=cfg.train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"], shuffle=False, batch_size=cfg.eval_batch_size\n",
    "    )\n",
    "    return train_dataloader, eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdafff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    set_seed(cfg.random_seed)\n",
    "    tokenized_datasets = tokenize_dataset()\n",
    "    \n",
    "    # Load metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    roc_auc_metric = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_checkpoint)\n",
    "\n",
    "    train_dataloader, eval_dataloader = create_dataloaders(tokenized_datasets)\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model_checkpoint, num_labels=cfg.num_labels\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=cfg.lora_r,\n",
    "        lora_alpha=cfg.lora_alpha,\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # Target attention layers\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters info\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.parameters(), eps=cfg.adam_epsilon, lr=cfg.lr\n",
    "    )\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=cfg.num_warmup_steps,\n",
    "        num_training_steps=len(train_dataloader) * cfg.epochs,\n",
    "    )\n",
    "    progress_bar = tqdm(\n",
    "        range(cfg.epochs * len(train_dataloader)),\n",
    "    )\n",
    "\n",
    "    best_macro_f1 = 0\n",
    "    checkpoint_dir = \"../results/checkpoints\"\n",
    "\n",
    "    # Model Training\n",
    "    for epoch in range(cfg.epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        all_labels = []\n",
    "\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "\n",
    "            all_predictions.append(accelerator.gather(predictions))\n",
    "            all_probabilities.append(accelerator.gather(probabilities))\n",
    "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)[\n",
    "            : len(tokenized_datasets[\"validation\"])\n",
    "        ]\n",
    "        all_probabilities = torch.cat(all_probabilities)[\n",
    "            : len(tokenized_datasets[\"validation\"])\n",
    "        ]\n",
    "        all_labels = torch.cat(all_labels)[: len(tokenized_datasets[\"validation\"])]\n",
    "\n",
    "        # Compute metrics\n",
    "        eval_accuracy = accuracy_metric.compute(\n",
    "            predictions=all_predictions, references=all_labels\n",
    "        )[\"accuracy\"]\n",
    "        eval_micro_f1 = f1_metric.compute(\n",
    "            predictions=all_predictions, references=all_labels, average=\"micro\"\n",
    "        )[\"f1\"]\n",
    "        eval_macro_f1 = f1_metric.compute(\n",
    "            predictions=all_predictions, references=all_labels, average=\"macro\"\n",
    "        )[\"f1\"]\n",
    "        eval_macro_auroc = roc_auc_metric.compute(\n",
    "            references=all_labels.cpu().numpy(),\n",
    "            prediction_scores=all_probabilities.cpu().numpy(),\n",
    "            multi_class=\"ovr\",\n",
    "            average=\"macro\"\n",
    "        )[\"roc_auc\"]\n",
    "\n",
    "        accelerator.print(\n",
    "            f\"epoch {epoch}: accuracy={eval_accuracy:.4f}, micro_f1={eval_micro_f1:.4f}, \"\n",
    "            f\"macro_f1={eval_macro_f1:.4f}, macro_auroc={eval_macro_auroc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save checkpoint if this is the best model based on macro AUROC\n",
    "        if eval_macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = eval_macro_f1\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            # Save only the LoRA adapters (much smaller than full model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f\"{checkpoint_dir}/roberta-large-lora-best\",\n",
    "                save_function=accelerator.save\n",
    "            )\n",
    "            # Also save the tokenizer for easy loading later\n",
    "            tokenizer.save_pretrained(f\"{checkpoint_dir}/roberta-large-lora-best\")\n",
    "            accelerator.print(f\"Saved new best LoRA adapter with macro_f1: {best_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ec58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Map: 100%|██████████| 3405/3405 [00:00<00:00, 9521.58 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 9599.21 examples/s]\n",
      "Map: 100%|██████████| 1065/1065 [00:00<00:00, 11087.37 examples/s]\n",
      "Loading weights: 100%|██████████| 389/389 [00:00<00:00, 473.99it/s, Materializing param=roberta.encoder.layer.23.output.dense.weight]              \n",
      "RobertaForSequenceClassification LOAD REPORT from: FacebookAI/roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,130,563 || all params: 363,493,382 || trainable%: 2.2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 107/535 [01:26<05:05,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: accuracy=0.7383, micro_f1=0.7383, macro_f1=0.5716, macro_auroc=0.8916\n",
      "Saved new best LoRA adapter with macro_f1: 0.5716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 214/535 [03:03<03:49,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: accuracy=0.8803, micro_f1=0.8803, macro_f1=0.8390, macro_auroc=0.9715\n",
      "Saved new best LoRA adapter with macro_f1: 0.8390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 321/535 [04:40<02:32,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: accuracy=0.8885, micro_f1=0.8885, macro_f1=0.8511, macro_auroc=0.9772\n",
      "Saved new best LoRA adapter with macro_f1: 0.8511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 428/535 [06:17<01:16,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: accuracy=0.8850, micro_f1=0.8850, macro_f1=0.8486, macro_auroc=0.9780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [07:53<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: accuracy=0.8920, micro_f1=0.8920, macro_f1=0.8522, macro_auroc=0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [08:04<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best LoRA adapter with macro_f1: 0.8522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## TRAINING\n",
    "training_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0c1d0",
   "metadata": {},
   "source": [
    "GPU memory usage:  10296MiB (10% less than fine-tuning)\n",
    "\n",
    "Trainig time ~ 8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c42d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3405/3405 [00:00<00:00, 11061.12 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 11412.67 examples/s]\n",
      "Map: 100%|██████████| 1065/1065 [00:00<00:00, 11107.55 examples/s]\n",
      "Downloading builder script: 7.56kB [00:00, 10.2MB/s]\n",
      "Downloading builder script: 7.38kB [00:00, 10.4MB/s]\n",
      "Loading weights: 100%|██████████| 389/389 [00:00<00:00, 580.72it/s, Materializing param=roberta.encoder.layer.23.output.dense.weight]              \n",
      "RobertaForSequenceClassification LOAD REPORT from: FacebookAI/roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Evaluating test set: 100%|██████████| 34/34 [00:12<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEST SET EVALUATION RESULTS\n",
      "==================================================\n",
      "Accuracy:          0.8704\n",
      "--------------------------------------------------\n",
      "F1 Scores:\n",
      "  Micro F1:        0.8704\n",
      "  Macro F1:        0.8194\n",
      "  Weighted F1:     0.8688\n",
      "--------------------------------------------------\n",
      "Precision:\n",
      "  Micro Precision: 0.8704\n",
      "  Macro Precision: 0.8309\n",
      "--------------------------------------------------\n",
      "Recall:\n",
      "  Micro Recall:    0.8704\n",
      "  Macro Recall:    0.8101\n",
      "--------------------------------------------------\n",
      "ROC-AUC:\n",
      "  Macro AUROC:     0.9715\n",
      "  Weighted AUROC:  0.9684\n",
      "==================================================\n",
      "\n",
      "Results saved to ../results/predictions/roberta_large_lora_predictions.pkl\n"
     ]
    }
   ],
   "source": [
    "## Save results from test set for evaluation later\n",
    "\n",
    "def evaluate_and_save_test_results(output_path=\"../results/predictions/roberta_large_lora_predictions.pkl\"):\n",
    "    \"\"\"Evaluate on test set and save results for later analysis.\"\"\"\n",
    "    accelerator = Accelerator()\n",
    "    set_seed(cfg.random_seed)\n",
    "    \n",
    "    # Recreate tokenized datasets\n",
    "    tokenized_datasets = tokenize_dataset()\n",
    "    \n",
    "    # Load metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    roc_auc_metric = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    \n",
    "    # Load the base model first\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model_checkpoint, num_labels=cfg.num_labels\n",
    "    )\n",
    "    \n",
    "    # Load the LoRA adapters on top of the base model\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        \"../results/checkpoints/roberta-large-lora-best\"\n",
    "    )\n",
    "    \n",
    "    # Create test dataloader\n",
    "    test_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"test\"], shuffle=False, batch_size=cfg.eval_batch_size\n",
    "    )\n",
    "    \n",
    "    model, test_dataloader = accelerator.prepare(model, test_dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating test set\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        \n",
    "        all_predictions.append(accelerator.gather(predictions).cpu().numpy())\n",
    "        all_labels.append(accelerator.gather(batch[\"labels\"]).cpu().numpy())\n",
    "        all_probabilities.append(accelerator.gather(probabilities).cpu().numpy())\n",
    "    \n",
    "    # Concatenate and trim to actual test set size\n",
    "    test_size = len(tokenized_datasets[\"test\"])\n",
    "    all_predictions = np.concatenate(all_predictions)[:test_size]\n",
    "    all_labels = np.concatenate(all_labels)[:test_size]\n",
    "    all_probabilities = np.concatenate(all_probabilities)[:test_size]\n",
    "    \n",
    "    # Compute all metrics\n",
    "    test_accuracy = accuracy_metric.compute(\n",
    "        predictions=all_predictions, references=all_labels\n",
    "    )[\"accuracy\"]\n",
    "    \n",
    "    test_micro_f1 = f1_metric.compute(\n",
    "        predictions=all_predictions, references=all_labels, average=\"micro\"\n",
    "    )[\"f1\"]\n",
    "    \n",
    "    test_macro_f1 = f1_metric.compute(\n",
    "        predictions=all_predictions, references=all_labels, average=\"macro\"\n",
    "    )[\"f1\"]\n",
    "    \n",
    "    test_weighted_f1 = f1_metric.compute(\n",
    "        predictions=all_predictions, references=all_labels, average=\"weighted\"\n",
    "    )[\"f1\"]\n",
    "    \n",
    "    test_macro_auroc = roc_auc_metric.compute(\n",
    "        references=all_labels,\n",
    "        prediction_scores=all_probabilities,\n",
    "        multi_class=\"ovr\",\n",
    "        average=\"macro\"\n",
    "    )[\"roc_auc\"]\n",
    "    \n",
    "    test_weighted_auroc = roc_auc_metric.compute(\n",
    "        references=all_labels,\n",
    "        prediction_scores=all_probabilities,\n",
    "        multi_class=\"ovr\",\n",
    "        average=\"weighted\"\n",
    "    )[\"roc_auc\"]\n",
    "    \n",
    "    test_micro_precision = precision_metric.compute(\n",
    "        predictions=all_predictions, references=all_labels, average=\"micro\"\n",
    "    )[\"precision\"]\n",
    "    \n",
    "    test_macro_precision = precision_metric.compute(\n",
    "        predictions=all_predictions, references=all_labels, average=\"macro\"\n",
    "    )[\"precision\"]\n",
    "    \n",
    "    test_micro_recall = recall_metric.compute(\n",
    "        predictions=all_predictions, references=all_labels, average=\"micro\"\n",
    "    )[\"recall\"]\n",
    "    \n",
    "    test_macro_recall = recall_metric.compute(\n",
    "        predictions=all_predictions, references=all_labels, average=\"macro\"\n",
    "    )[\"recall\"]\n",
    "    \n",
    "    # Print all metrics\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TEST SET EVALUATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Accuracy:          {test_accuracy:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"F1 Scores:\")\n",
    "    print(f\"  Micro F1:        {test_micro_f1:.4f}\")\n",
    "    print(f\"  Macro F1:        {test_macro_f1:.4f}\")\n",
    "    print(f\"  Weighted F1:     {test_weighted_f1:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Precision:\")\n",
    "    print(f\"  Micro Precision: {test_micro_precision:.4f}\")\n",
    "    print(f\"  Macro Precision: {test_macro_precision:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Recall:\")\n",
    "    print(f\"  Micro Recall:    {test_micro_recall:.4f}\")\n",
    "    print(f\"  Macro Recall:    {test_macro_recall:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"ROC-AUC:\")\n",
    "    print(f\"  Macro AUROC:     {test_macro_auroc:.4f}\")\n",
    "    print(f\"  Weighted AUROC:  {test_weighted_auroc:.4f}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Save results using pickle (consistent with ml_baselines notebook)\n",
    "    import pickle\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    results = {\n",
    "        \"RoBERTa-large-LoRA\": all_predictions,\n",
    "        \"y_true\": all_labels,\n",
    "        \"probabilities\": all_probabilities,\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": test_accuracy,\n",
    "            \"micro_f1\": test_micro_f1,\n",
    "            \"macro_f1\": test_macro_f1,\n",
    "            \"weighted_f1\": test_weighted_f1,\n",
    "            \"micro_precision\": test_micro_precision,\n",
    "            \"macro_precision\": test_macro_precision,\n",
    "            \"micro_recall\": test_micro_recall,\n",
    "            \"macro_recall\": test_macro_recall,\n",
    "            \"macro_auroc\": test_macro_auroc,\n",
    "            \"weighted_auroc\": test_weighted_auroc,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_path}\")\n",
    "    \n",
    "    return all_predictions, all_labels, all_probabilities\n",
    "\n",
    "# Run evaluation and save\n",
    "predictions, labels, probabilities = evaluate_and_save_test_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bede7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin_sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
